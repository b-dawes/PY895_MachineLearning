{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: Using Deep Learning to Study SUSY with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "The goal of this notebook is to introduce the powerful PyTorch framework for building neural networks and use it to analyze the SUSY dataset. After this notebook, the reader should understand the mechanics of PyTorch and how to construct DNNs using this package. In addition, the reader is encouraged to explore the GPU backend available in Pytorch on this dataset.\n",
    "\n",
    "## Overview\n",
    "In this notebook, we use Deep Neural Networks to classify the supersymmetry dataset, first introduced by Baldi et al. in [Nature Communication (2015)](https://www.nature.com/articles/ncomms5308). The SUSY data set consists of 5,000,000 Monte-Carlo samples of supersymmetric and non-supersymmetric collisions with $18$ features. The signal process is the production of electrically-charged supersymmetric particles which decay to $W$ bosons and an electrically-neutral supersymmetric particle that is invisible to the detector.\n",
    "\n",
    "The first $8$ features are \"raw\" kinematic features that can be directly measured from collisions. The final $10$ features are \"hand constructed\" features that have been chosen using physical knowledge and are known to be important in distinguishing supersymmetric and non-supersymmetric collision events. More specifically, they are given by the column names below.\n",
    "\n",
    "In this notebook, we study this dataset using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13ac03e4e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Procedure\n",
    "\n",
    "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
    "\n",
    "* ***step 1:*** Load and process the data\n",
    "* ***step 2:*** Define the model and its architecture\n",
    "* ***step 3:*** Choose the optimizer and the cost function\n",
    "* ***step 4:*** Train the model \n",
    "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
    "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
    "\n",
    "Below, we sometimes combine some of these steps together for convenience.\n",
    "\n",
    "Notice that we take a rather different approach, compared to the simpler MNIST Keras notebook. We first define a set of classes and functions and run the actual computation only in the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the SUSY Dataset\n",
    "\n",
    "The supersymmetry dataset can be downloaded from the UCI Machine Learning repository on [https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz](https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz). The dataset is quite large. Download the dataset and unzip it in a directory.\n",
    "\n",
    "Loading data in Pytroch is done by creating a user-defined a class, which we name `SUSY_Dataset`, and is a child of the `torch.utils.data.Dataset` class. This ensures that all necessary attributes required for the processing of the data during the training and test stages are easily inherited. The `__init__` method of our custom data class should contain the usual code for loading the data, which is problem-specific, and has been discussed for the SUSY data set in Notebook 5. More importantly, the user-defined data class must override the `__len__` and `__getitem__` methods of the parent `DataSet` class. The former returns the size of the data set, while the latter allows the user to access a particular data point from the set by specifying its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets # load data\n",
    "\n",
    "class SUSY_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"SUSY pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file, root_dir, dataset_size, train=True, transform=None, high_level_feats=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            train (bool, optional): If set to `True` load training data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            high_level_festures (bool, optional): If set to `True`, working with high-level features only. \n",
    "                                        If set to `False`, working with low-level features only.\n",
    "                                        Default is `None`: working with all features\n",
    "        \"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        features=['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2', \n",
    "                'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "        low_features=['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi']\n",
    "\n",
    "        high_features=['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "\n",
    "        #Number of datapoints to work with\n",
    "        df = pd.read_csv(root_dir+data_file, header=None,nrows=dataset_size,engine='python')\n",
    "        df.columns=features\n",
    "        Y = df['SUSY']\n",
    "        X = df[[col for col in df.columns if col!=\"SUSY\"]]\n",
    "\n",
    "        # set training and test data size\n",
    "        train_size=int(0.8*dataset_size)\n",
    "        self.train=train\n",
    "\n",
    "        if self.train:\n",
    "            X=X[:train_size]\n",
    "            Y=Y[:train_size]\n",
    "            print(\"Training on {} examples\".format(train_size))\n",
    "        else:\n",
    "            X=X[train_size:]\n",
    "            Y=Y[train_size:]\n",
    "            print(\"Testing on {} examples\".format(dataset_size-train_size))\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # make datasets using only the 8 low-level features and 10 high-level features\n",
    "        if high_level_feats is None:\n",
    "            self.data=(X.values.astype(np.float32),Y.values.astype(np.long))\n",
    "            print(\"Using both high and low level features\")\n",
    "        elif high_level_feats is True:\n",
    "            self.data=(X[high_features].values.astype(np.float32),Y.values)\n",
    "            print(\"Using both high-level features only.\")\n",
    "        elif high_level_feats is False:\n",
    "            self.data=(X[low_features].values.astype(np.float32),Y.values)\n",
    "            print(\"Using both low-level features only.\")\n",
    "\n",
    "\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we define a helper function `load_data()` that accepts as a required argument the set of parameters `args`, and returns two generators: `test_loader` and `train_loader` which readily return mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "\n",
    "    data_file='SUSY.csv.gz'\n",
    "    root_dir='../../Data/SUSY/'\n",
    "\n",
    "    kwargs = {} # CUDA arguments, if enabled\n",
    "    # load and noralise train and test data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=True,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=False,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "To construct neural networks with Pytorch, we make another class called `model` as a child of Pytorch's `nn.Module` class. The `model` class initializes the types of layers needed for the deep neural net in its `__init__` method, while the DNN is assembled in a function method called `forward`, which accepts an `autograd.Variable` object and returns the output layer. Using this convention Pytorch will automatically recognize the structure of the DNN, and the `autograd` module will pull the gradients forward and backward using backprop.\n",
    "\n",
    "Our code below is constructed in such a way that one can choose whether to use the high-level and low-level features separately and altogether. This choice determines the size of the fully-connected input layer `fc1`. Therefore the `__init__` method accepts the optional argument `high_level_feats`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self,high_level_feats=None):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        if high_level_feats is None:\n",
    "            self.fc1 = nn.Linear(18, 200) # all features\n",
    "        elif high_level_feats:\n",
    "            self.fc1 = nn.Linear(10, 200) # low-level only\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(8, 200) # high-level only\n",
    "\n",
    "\n",
    "        self.batchnorm1=nn.BatchNorm1d(200, eps=1e-05, momentum=0.1)\n",
    "        self.batchnorm2=nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(200, 100) # see forward function for dimensions\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feed-forward function for the NN.\n",
    "\n",
    "        A backward function is automatically defined using `torch.autograd`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : autograd.Tensor\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        autograd.Tensor\n",
    "            output layer of NN\n",
    "\n",
    "        '''\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply dropout\n",
    "        x=self.batchnorm1(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply dropout\n",
    "        x=self.batchnorm2(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply affine operation fc2\n",
    "        x = self.fc3(x)\n",
    "        # soft-max layer\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3+4+5: Choose the Optimizer and the Cost Function. Train and Evaluate the Model\n",
    "\n",
    "Next, we define the function `evaluate_model`. The first argument, `args`, contains all hyperparameters needed for the DNN (see below). The second and third arguments are the `train_loader` and the `test_loader` objects, returned by the function `load_data()` we defined in Step 1 above. The `evaluate_model` function returns the final `test_loss` and `test_accuracy` of the model.\n",
    "\n",
    "First, we initialize a `model` and call the object `DNN`. In order to define the loss function and the optimizer, we use modules `torch.nn.functional` (imported here as `F`) and `torch.optim`. As a loss function we choose the negative log-likelihood, and stored is under the variable `criterion`. As usual, we can choose any from a variety of different SGD-based optimizers, but we focus on the traditional SGD.\n",
    "\n",
    "Next, we define two functions: `train()` and `test()`. They are called at the end of `evaluate_model` where we loop over the training epochs to train and test our model. \n",
    "\n",
    "The `train` function accepts an integer called `epoch`, which is only used to print the training data. We first set the `DNN` in a train mode using the `train()` method inherited from `nn.Module`. Then we loop over the mini-batches in `train_loader`. We cast the data as pytorch `Variable`, re-set the `optimizer`, perform the forward step by calling the `DNN` model on the `data` and computing the `loss`. The backprop algorithm is then easily done using the `backward()` method of the loss function `criterion`. We use `optimizer.step` to update the weights of the `DNN`. Last print the performance for every minibatch. `train` returns the loss on the data.\n",
    "\n",
    "The `test` function is similar to `train` but its purpose is to test the performance of a trained model. Once we set the `DNN` model in `eval()` mode, the following steps are similar to those in `train`. We then compute the `test_loss` and the number of `correct` predictions, print the results and return them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "def evaluate_model(args,train_loader,test_loader):\n",
    "\n",
    "    # create model\n",
    "    DNN = model(high_level_feats=args.high_level_feats)\n",
    "    DNN = DNN.to(torch.device(\"cuda\"))\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define SGD optimizer\n",
    "    #optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    optimizer = optim.Adam(DNN.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def train(epoch):\n",
    "        '''Trains a NN using minibatches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Training epoch number.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # set model to training mode (affects Dropout and BatchNorm)\n",
    "        DNN.train()\n",
    "        # loop over training data\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            # convert labels to longs\n",
    "            label = label.type(torch.LongTensor)\n",
    "            # send data to gpu\n",
    "            label = label.to(torch.device(\"cuda\"))\n",
    "            data = data.to(torch.device(\"cuda\"))\n",
    "            # zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            # compute output of final layer: forward step\n",
    "            output = DNN(data)\n",
    "            # compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # run backprop: backward step\n",
    "            loss.backward()\n",
    "            # update weigths of NN\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss at current epoch\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() ))\n",
    "            \n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def test():\n",
    "        '''Tests NN performance.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # evaluate model\n",
    "        DNN.eval()\n",
    "\n",
    "        test_loss = 0 # loss function on test data\n",
    "        correct = 0 # number of correct predictions\n",
    "        # loop over test data\n",
    "        for data, label in test_loader:\n",
    "            # convert labels to longs\n",
    "            label = label.type(torch.LongTensor)\n",
    "            # send data to gpu\n",
    "            label = label.to(torch.device(\"cuda\"))\n",
    "            data = data.to(torch.device(\"cuda\"))\n",
    "            # compute model prediction softmax probability\n",
    "            output = DNN(data)\n",
    "            # compute test loss\n",
    "            test_loss += criterion(output, label, size_average=False).item() # sum up batch loss\n",
    "            # find most likely prediction\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # update number of correct predictions\n",
    "            correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        # print test loss\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "\n",
    "        return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    train_loss=np.zeros((args.epochs,))\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    epochs=range(1, args.epochs + 1)\n",
    "    for epoch in epochs:\n",
    "\n",
    "        train_loss[epoch-1] = train(epoch)\n",
    "        test_loss[epoch-1], test_accuracy[epoch-1] = test()\n",
    "\n",
    "\n",
    "\n",
    "    return test_loss[-1], test_accuracy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "To study the performance of the model for a variety of different `data_set_sizes` and `learning_rates`, we do a grid search. \n",
    "\n",
    "Let us define a function `grid_search`, which accepts the `args` variable containing all hyper-parameters needed for the problem. After choosing logarithmically-spaced `data_set_sizes` and `learning_rates`, we first loop over all `data_set_sizes`, update the `args` variable, and call the `load_data` function. We then loop once again over all `learning_rates`, update `args` and call `evaluate_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(args):\n",
    "\n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    dataset_size=200000 #[1000, 10000, 100000, 200000] #np.logspace(2,5,4).astype('int')\n",
    "    args.dataset_size=dataset_size\n",
    "    args.batch_size=int(0.01*dataset_size)\n",
    "    learning_rates=[.001,.01,0.1]#np.logspace(-5,-1,5)\n",
    "    momenta = [0.1]\n",
    "    \n",
    "    args.high_level_feats=True\n",
    "    \n",
    "    train_loader, test_loader = load_data(args)\n",
    "\n",
    "\n",
    "    # pre-alocate data\n",
    "    test_loss=np.zeros((len(momenta),len(learning_rates)),dtype=np.float64)\n",
    "    test_accuracy=np.zeros_like(test_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, momentum in enumerate(momenta):\n",
    "        # upate data set size parameters\n",
    "        args.momentum=momentum\n",
    "\n",
    "        # load data\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            # update learning rate\n",
    "            args.lr=lr\n",
    "\n",
    "            print(\"\\n training DNN with %5d data points and SGD lr=%0.6f. \\n\" %(dataset_size,lr) )\n",
    "\n",
    "            test_loss[i,j],test_accuracy[i,j] = evaluate_model(args,train_loader,test_loader)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,momenta,test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we use the function `plot_data`, defined below, to plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,0.6,0.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{momentum}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Code\n",
    "\n",
    "As we mentioned in the beginning of the notebook, all functions and classes discussed above only specify the procedure but do not actually perform any computations. This allows us to re-use them for different problems. \n",
    "\n",
    "Actually running the training and testing for every point in the grid search is done below. The `argparse` class allows us to conveniently keep track of all hyperparameters, stored in the variable `args` which enters most of the functions we defined above. \n",
    "\n",
    "To run the simulation, we call the function `grid_search`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* One of the advantages of Pytorch is that it allows to automatically use the CUDA library for fast performance on GPU's. For the sake of clarity, we have omitted this in the above notebook. Go online to check how to put the CUDA commands back into the code above. _Hint:_ study the [Pytorch MNIST tutorial](https://github.com/pytorch/examples/blob/master/mnist/main.py) to see how this works in practice.\n",
    "\n",
    "I added in the use of CUDA to the above notebook. I had to play around with a bunch of graphic driver settings on my laptop but I eventually got it working.\n",
    "\n",
    "I also ran into the issue with long vs int data types. I think the issue is that negative log likelihood expects its labels to be longs (I'm not sure why). I fixed this by explicitly casting to longs by using `labels.type(torch.LongTensor)` in the train and test methods.\n",
    "\n",
    "I tried playing around to improve the accuracy but wasn't able to change results from about 80%. I adjusted the dataset size, learning rate, and momentum. I also switched from SGD to ADAM, switched from Dropout to Batchnorm, and changed the parameter set I was using. The dataset size and learning rate seemed to have the most effect out of all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 160000 examples\n",
      "Using both high-level features only.\n",
      "Testing on 40000 examples\n",
      "Using both high-level features only.\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.864256\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.474406\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.480847\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.440060\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.453315\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.436256\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.410253\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.453792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\Anaconda3\\envs\\ML_PY895\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4492, Accuracy: 31653/40000 (79.132%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.445485\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.463663\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.447305\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.438751\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.430553\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.451729\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.438544\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.446781\n",
      "\n",
      "Test set: Average loss: 0.4488, Accuracy: 31574/40000 (78.935%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.446251\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.462072\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.448734\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.435128\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.442363\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.428249\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.451335\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.469246\n",
      "\n",
      "Test set: Average loss: 0.4454, Accuracy: 31723/40000 (79.308%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.437549\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.445857\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.453972\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.438191\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.446472\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.476095\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.454697\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.449425\n",
      "\n",
      "Test set: Average loss: 0.4462, Accuracy: 31669/40000 (79.172%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.454857\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.452885\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.437273\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.457791\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.445052\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.451987\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.449187\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.457848\n",
      "\n",
      "Test set: Average loss: 0.4467, Accuracy: 31630/40000 (79.075%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.454376\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.456322\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.440373\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.445300\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.444779\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.449819\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.447209\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.434192\n",
      "\n",
      "Test set: Average loss: 0.4437, Accuracy: 31737/40000 (79.343%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.424091\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.430057\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.426522\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.432204\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.426039\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.442980\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.451379\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.429573\n",
      "\n",
      "Test set: Average loss: 0.4440, Accuracy: 31763/40000 (79.407%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.434308\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.451015\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.436646\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.485894\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.442302\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.444211\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.453639\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.415801\n",
      "\n",
      "Test set: Average loss: 0.4466, Accuracy: 31642/40000 (79.105%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.441743\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.450846\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.434937\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.452958\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.468877\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.434537\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.439623\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.443415\n",
      "\n",
      "Test set: Average loss: 0.4437, Accuracy: 31751/40000 (79.377%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.445979\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.433446\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.446648\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.439991\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.455627\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.424663\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.433647\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.441797\n",
      "\n",
      "Test set: Average loss: 0.4432, Accuracy: 31759/40000 (79.397%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.793249\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.487431\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.482890\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.469614\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.435251\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.451302\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.459094\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.462756\n",
      "\n",
      "Test set: Average loss: 0.4622, Accuracy: 31282/40000 (78.205%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.447801\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.461419\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.455931\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.458293\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.449941\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.447483\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.482847\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.447297\n",
      "\n",
      "Test set: Average loss: 0.4485, Accuracy: 31651/40000 (79.127%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.463189\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.456821\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.437837\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.446710\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.449968\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.454449\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.455374\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.456357\n",
      "\n",
      "Test set: Average loss: 0.4585, Accuracy: 31432/40000 (78.580%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.440604\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.428182\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.433574\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.465021\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.469259\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.444383\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.456101\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.459462\n",
      "\n",
      "Test set: Average loss: 0.4516, Accuracy: 31561/40000 (78.903%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.445587\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.447775\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.437980\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.457174\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.435483\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.443935\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.438484\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.452666\n",
      "\n",
      "Test set: Average loss: 0.4485, Accuracy: 31672/40000 (79.180%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.416623\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.446734\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.443031\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.449828\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.447569\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.460931\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.450756\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.429033\n",
      "\n",
      "Test set: Average loss: 0.4500, Accuracy: 31574/40000 (78.935%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.457384\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.461040\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.450494\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.454805\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.463567\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.438468\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.442134\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.433667\n",
      "\n",
      "Test set: Average loss: 0.4445, Accuracy: 31740/40000 (79.350%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.441811\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.441172\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.467885\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.453350\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.451981\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.462318\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.454211\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.457212\n",
      "\n",
      "Test set: Average loss: 0.4432, Accuracy: 31769/40000 (79.422%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.430740\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.430233\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.454967\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.443931\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.426815\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.431323\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.443261\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.439763\n",
      "\n",
      "Test set: Average loss: 0.4499, Accuracy: 31624/40000 (79.060%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.448734\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.442409\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.428138\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.456416\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.422283\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.434632\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.465367\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.439769\n",
      "\n",
      "Test set: Average loss: 0.4461, Accuracy: 31677/40000 (79.192%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.726157\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.523438\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.490234\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.470552\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.457217\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.461045\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.491325\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.460169\n",
      "\n",
      "Test set: Average loss: 0.4604, Accuracy: 31437/40000 (78.593%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.480284\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.468254\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.458685\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.449535\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.458888\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.465071\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.467433\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.450820\n",
      "\n",
      "Test set: Average loss: 0.4565, Accuracy: 31505/40000 (78.763%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.431466\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.443096\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.484829\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.459583\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.456184\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.443877\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.468159\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.454175\n",
      "\n",
      "Test set: Average loss: 0.4547, Accuracy: 31542/40000 (78.855%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.460286\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.450574\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.451207\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.459856\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.443067\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.436389\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.455310\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.453677\n",
      "\n",
      "Test set: Average loss: 0.4523, Accuracy: 31525/40000 (78.812%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.441015\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.435931\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.466747\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.475700\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.442418\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.444478\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.452992\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.445635\n",
      "\n",
      "Test set: Average loss: 0.4544, Accuracy: 31540/40000 (78.850%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.456852\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.444931\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.451473\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.447810\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.434889\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.473989\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.448591\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.445650\n",
      "\n",
      "Test set: Average loss: 0.4489, Accuracy: 31715/40000 (79.287%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.442015\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.468352\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.452682\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.458847\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.466846\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.438592\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.440779\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.447812\n",
      "\n",
      "Test set: Average loss: 0.4867, Accuracy: 30524/40000 (76.310%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.443965\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.454508\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.452965\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.453977\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.434375\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.453154\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.459169\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.429815\n",
      "\n",
      "Test set: Average loss: 0.4516, Accuracy: 31531/40000 (78.828%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.460190\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.448682\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.467293\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.444245\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.462069\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.439778\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.453646\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.441744\n",
      "\n",
      "Test set: Average loss: 0.4505, Accuracy: 31677/40000 (79.192%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.452158\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.458792\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.450001\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.464553\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.451246\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.451187\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.434761\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.464509\n",
      "\n",
      "Test set: Average loss: 0.4629, Accuracy: 31221/40000 (78.052%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch SUSY Example')\n",
    "parser.add_argument('--dataset_size', type=int, default=100000, metavar='DS',\n",
    "                help='size of data set (default: 100000)')\n",
    "parser.add_argument('--high_level_feats', type=bool, default=None, metavar='HLF',\n",
    "                help='toggles high level features (default: None)')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.05, metavar='LR',\n",
    "                help='learning rate (default: 0.02)')\n",
    "parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n",
    "                help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# set seed of random number generator\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "grid_search(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
