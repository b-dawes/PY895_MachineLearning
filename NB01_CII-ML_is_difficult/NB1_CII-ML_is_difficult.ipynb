{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Why is Machine Learning difficult?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "In this notebook, we will get our hands dirty trying to gain intuition about why machine learning is difficult. \n",
    "\n",
    "Our task is going to be a simple one, fitting data with polynomials of different order. Formally, this goes under the name of polynomial regression. Here we will do a series of exercises that are intended to give the reader intuition about the major challenges that any machine learning algorithm faces.\n",
    "\n",
    "## Learning Goal\n",
    "\n",
    "We will explore how our ability to predict depends on the number of data points we have, the \"noise\" in the data, and our knowledge about relevant features. The goal is to build intuition about why prediction is difficult and discuss general strategies for overcoming these difficulties.\n",
    "\n",
    "\n",
    "## The Prediction Problem\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The data is generated by drawing samples from the equation\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable such that\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \\\\\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "We will refer to the $f(x_i)$ as the **true features** used to generate the data. \n",
    "\n",
    "To make prediction, we will consider a family of functions $g_\\alpha(x;\\theta_\\alpha)$ that depend on some parameters $\\theta_\\alpha$. These functions respresent the **model class** that we are using to try to model the data and make predictions. The $g_\\alpha(x;\\theta_\\alpha)$ encode the class of **features** we are using to represent the data.\n",
    "\n",
    "To learn the parameters $\\boldsymbol{\\theta}$, we will train our models on a **training data set** and then test the effectiveness of the model on a <i>different</i> dataset, the **test data set**. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen. As we will see below, models that give the best fit to the training data do not necessarily make the best predictions on the test data. This will be a running theme that we will encounter repeatedly in machine learning.  \n",
    "\n",
    "\n",
    "For the remainder of the notebook, we will focus on polynomial regression. Our task is to model the data with polynomials and make predictions about the new data that we have not seen.\n",
    "We will consider two qualitatively distinct situations: \n",
    " \n",
    "- In the first case, the process that generates the underlying data is in the model class we are using to make predictions. For polynomial regression, this means that the functions $f(x_i)$ are themselves polynomials.\n",
    "- In the second case, our data lies outside our model class. In the case of polynomial regression, this could correspond to the case where the $f(x_i)$ is a 10-th order polynomial but $g_\\alpha(x;\\theta_\\alpha)$ are polynomials of order 1 or 3.\n",
    " \n",
    "\n",
    "In the exercises and discussion we consider 3 model classes:\n",
    " \n",
    "- the case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 1 (linear models),\n",
    "- the case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 3,\n",
    "- the case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 10.\n",
    "\n",
    "\n",
    "To measure our ability to predict, we will learn our parameters by fitting our training dataset and then making predictions on our test data set. One common measure of predictive  performance of our algorithm is to compare the predictions,$\\{y_j^\\mathrm{pred}\\}$, to the true values $\\{y_j\\}$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n",
    "$$\n",
    "MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n",
    "$$\n",
    "We will return to this in later notebooks. For now, we will try to get a qualitative picture by examining plots on test and training data.\n",
    "\n",
    "## Fitting vs. predicting when the data is in the model class\n",
    "\n",
    "\n",
    "We start by considering the case:\n",
    "$$\n",
    "f(x)=2x.\n",
    "$$\n",
    "Then the data is clearly generated by a model that is contained within all three model classes we are using to make predictions (linear models, third order polynomials, and tenth order polynomials). \n",
    "\n",
    "\n",
    "Run the code for the following cases:\n",
    "\n",
    "- For $f(x)=2x$, $N_{\\mathrm{train}}=10$ and $\\sigma=0$ (noiseless case), train the three classes of models (linear, third-order polynomial, and tenth order polynomial) for a training set when $x_i  \\in [0,1]$. Make graphs comparing fits for different order of polynomials. Which model fits the data the best?\n",
    "- Do you think that the data that has the least error on the training set will also make the best predictions? Why or why not? Can you try to discuss and formalize your intuition? What can go right and what can go wrong?\n",
    "- Check your answer by seeing how well your fits predict newly generated test data (including on data outside the range you fit on, for example $x \\in [0,1.2]$) using the code below. How well do you do on points in the range of $x$ where you trained the model? How about points outside the original training data set? \n",
    "- Repeat the exercises above for $f(x)=2x$, $N_{\\mathrm{train}}=10$, and $\\sigma=1$. What changes?\n",
    "- Repeat the exercises above for $f(x)=2x$, $N_{\\mathrm{train}}=100$, and $\\sigma=1$. What changes?\n",
    "- Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well.\n",
    "\n",
    "## Fitting vs. predicting when the data is not in the model class\n",
    "Thus far, we have considered the case where the data is generated using a model contained in the model class. Now consider  $f(x)=2x-10x^5+15x^{10}$. *Notice that the for linear and third-order polynomial the true model $f(x)$ is not contained in model class $g_\\alpha(x)$* .\n",
    "\n",
    " \n",
    "- Repeat the exercises above fitting and predicting for $f(x)=2x-10x^5+15x^{10}$ for $N_{\\mathrm{train}}=10,100$ and $\\sigma=0,1$. Record your observations.\n",
    "- Do better fits lead to better predictions?\n",
    "- What is the relationship between the true model for generating the data and the model class that has the most predictive power? How is this related to the model complexity? How does this depend on the number of data points $N_{\\mathrm{train}}$ and $\\sigma$?\n",
    "- Summarize what you think you learned about the relationship of knowing the true model class and predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#This is Python Notebook to walk through polynomial regression examples\n",
    "#We will use this to think about regression\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "fig = plt.figure(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The Training Data\n",
    "N_train=10\n",
    "sigma_train=1\n",
    "\n",
    "# Train on integers\n",
    "x=np.linspace(0.05,0.95,N_train)\n",
    "# Draw Gaussian random noise\n",
    "s = sigma_train*np.random.randn(N_train)\n",
    "\n",
    "#linear\n",
    "#y=2*x+s\n",
    "\n",
    "# Tenth Order\n",
    "y=2*x-10*x**5+15*x**10+s\n",
    "\n",
    "p1=plt.plot(x, y, \"o\", ms=8, alpha=0.5, label='Training')\n",
    "\n",
    "# Linear Regression : create linear regression object\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training set\n",
    "# Note: sklearn requires a design matrix of shape (N_train, N_features). Thus we reshape x to (N_train, 1):\n",
    "clf.fit(x[:, np.newaxis], y)\n",
    "\n",
    "# Use fitted linear model to predict the y value:\n",
    "xplot=np.linspace(0.02,0.98,200) # grid of points, some are in the training set, some are not\n",
    "linear_plot=plt.plot(xplot, clf.predict(xplot[:, np.newaxis]), label='Linear')\n",
    "\n",
    "# Polynomial Regression\n",
    "poly3 = PolynomialFeatures(degree=3)\n",
    "# Construct polynomial features\n",
    "X = poly3.fit_transform(x[:,np.newaxis])\n",
    "clf3 = linear_model.LinearRegression()\n",
    "clf3.fit(X,y)\n",
    "\n",
    "\n",
    "Xplot=poly3.fit_transform(xplot[:,np.newaxis])\n",
    "poly3_plot=plt.plot(xplot, clf3.predict(Xplot), label='Poly 3')\n",
    "\n",
    "# Fifth order polynomial in case you want to try it out\n",
    "#poly5 = PolynomialFeatures(degree=5)\n",
    "#X = poly5.fit_transform(x[:,np.newaxis])\n",
    "#clf5 = linear_model.LinearRegression()\n",
    "#clf5.fit(X,y)\n",
    "\n",
    "#Xplot=poly5.fit_transform(xplot[:,np.newaxis])\n",
    "#plt.plot(xplot, clf5.predict(Xplot), 'r--',linewidth=1)\n",
    "\n",
    "poly10 = PolynomialFeatures(degree=10)\n",
    "X = poly10.fit_transform(x[:,np.newaxis])\n",
    "clf10 = linear_model.LinearRegression()\n",
    "clf10.fit(X,y)\n",
    "\n",
    "Xplot=poly10.fit_transform(xplot[:,np.newaxis])\n",
    "poly10_plot=plt.plot(xplot, clf10.predict(Xplot), label='Poly 10')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.ylim([-7,7])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "Title=\"N=%i, $\\sigma=%.2f$\"%(N_train,sigma_train)\n",
    "plt.title(Title+\" (train)\")\n",
    "\n",
    "\n",
    "# Linear Filename\n",
    "#filename_train=\"train-linear_N=%i_noise=%.2f.pdf\"%(N_train, sigma_train)\n",
    "\n",
    "# Tenth Order Filename\n",
    "filename_train=\"train-o10_N=%i_noise=%.2f.pdf\"%(N_train, sigma_train)\n",
    "\n",
    "# Saving figure and showing results\n",
    "plt.savefig(filename_train)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate Test Data\n",
    "%matplotlib inline\n",
    "# Number of test data\n",
    "N_test=100\n",
    "sigma_test=sigma_train\n",
    "\n",
    "# Generate random grid points (x) in the interval [0, max_x]:\n",
    "# Note some points will be drawn outside the training interval\n",
    "max_x=1.2\n",
    "x_test=max_x*np.random.random(N_test)\n",
    "\n",
    "# Draw random Gaussian noise\n",
    "s_test = sigma_test*np.random.randn(N_test)\n",
    "\n",
    "# Linear\n",
    "#y_test=2*x_test+s_test\n",
    "# Tenth order\n",
    "y_test=2*x_test-10*x_test**5+15*x_test**10+s_test\n",
    "\n",
    "# Make design matrices for prediction\n",
    "x_plot=np.linspace(0,max_x, 200)\n",
    "X3 = poly3.fit_transform(x_plot[:,np.newaxis])\n",
    "X10 = poly10.fit_transform(x_plot[:,np.newaxis])\n",
    "\n",
    "############## PLOTTING RESULTS ##########\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "p1=plt.plot(x_test, y_test, 'o', ms=10, alpha=0.5, label='test data')\n",
    "p2=plt.plot(x_plot,clf.predict(x_plot[:,np.newaxis]), lw=2, label='linear')\n",
    "p3=plt.plot(x_plot,clf3.predict(X3), lw=2, label='3rd order')\n",
    "p10=plt.plot(x_plot,clf10.predict(X10), lw=2, label='10th order')\n",
    "\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "Title=\"N=%i, $\\sigma=%.2f$\"%(N_test,sigma_test)\n",
    "plt.title(Title+\" (pred.)\")\n",
    "plt.tight_layout()\n",
    "plt.ylim((-6,12))\n",
    "\n",
    "# Linear Filename\n",
    "#filename_test=\"pred-linear_N=%i_noise=%.2f.pdf\"%(N_train, sigma_test)\n",
    "\n",
    "# Tenth Order Filename\n",
    "filename_test=\"pred-o10_N=%i_noise=%.2f.pdf\"%(N_train, sigma_test)\n",
    "\n",
    "# Saving figure and showing results\n",
    "plt.savefig(filename_test)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "For 𝑓(𝑥)=2𝑥, 𝑁train=10 and 𝜎=0 (noiseless case), train the three classes of models (linear, third-order polynomial, and tenth order polynomial) for a training set when 𝑥𝑖∈[0,1]\n",
    ". Make graphs comparing fits for different order of polynomials. Which model fits the data the best?\n",
    "Do you think that the data that has the least error on the training set will also make the best predictions? Why or why not? Can you try to discuss and formalize your intuition? What can go right and what can go wrong? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Responses\n",
    "\n",
    "## Fitting vs. predicting when the data is in the model class\n",
    "**For $f(x)=2x$, $N_{\\mathrm{train}}=10$ and $\\sigma=0$ (noiseless case), train the three classes of models (linear, third-order polynomial, and tenth order polynomial) for a training set when $x_i  \\in [0,1]$. Make graphs comparing fits for different order of polynomials. Which model fits the data the best?**\n",
    "\n",
    "Since there is no noise, all models are essentially the same and fit equally well.\n",
    "\n",
    "**Do you think that the data that has the least error on the training set will also make the best predictions? Why or why not? Can you try to discuss and formalize your intuition? What can go right and what can go wrong?**\n",
    "\n",
    "All models have no error. All of them should make the same predictions because they all solved for the model exactly. This is because the model generating the data is contained within all of the models and there is no noise to throw off the fitting. As long as there is no noise, there should be no issues.\n",
    "\n",
    "**Check your answer by seeing how well your fits predict newly generated test data (including on data outside the range you fit on, for example $x \\in [0,1.2]$) using the code below. How well do you do on points in the range of $x$ where you trained the model? How about points outside the original training data set?**\n",
    "\n",
    "As the data has no noise, all models are able to extrapolate perfectly.\n",
    "\n",
    "**Repeat the exercises above for $f(x)=2x$, $N_{\\mathrm{train}}=10$, and $\\sigma=1$. What changes?**\n",
    "\n",
    "Now that there is noise, the higher order models end up fitting the data in the training set better than the linear model. The 10th order polynomial in fact fits perfectly. However, once we try applying it to different data, the linear model performs best by far. The 10th order polynomial performs horrible, especially when extrapolating outside of the training set. This is because the higher order polynomials are fitting to the noise in the training set as well as the underlying model. The cubic fits okay within the training window however it also performs horribly at extrapolating the data.\n",
    "\n",
    "**Repeat the exercises above for $f(x)=2x$, $N_{\\mathrm{train}}=100$, and $\\sigma=1$. What changes?**\n",
    "\n",
    "Similar to before, the 10th order model fits the training data best, although it no longer has exactly zero error. The cubic and linear models fit nearly the same in the training data. Once again, the 10th order model fails horribly on predicting data, especially when extrapolating out of the training set. The cubic model now actually fits pretty well in the extrapolated region. However, if we extrapolated further it would begin to diverge from the true model.\n",
    "\n",
    "**Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well.**\n",
    "\n",
    "Increasing the model complexity will generally increase the goodness of fit on training data. However, the goodness of fit on the training data does not neccessarily predict how well the fitting will do on new data. It is also hard to know how well a fit will work for data outside of the training set. Even if the fit works well within the domain of the training set, it can fit horribly once it starts extrapolating to a new domain.\n",
    "\n",
    "\n",
    "## Fitting vs. predicting when the data is not in the model class\n",
    "Thus far, we have considered the case where the data is generated using a model contained in the model class. Now consider  $f(x)=2x-10x^5+15x^{10}$. *Notice that the for linear and third-order polynomial the true model $f(x)$ is not contained in model class $g_\\alpha(x)$* .\n",
    "\n",
    "**Repeat the exercises above fitting and predicting for $f(x)=2x-10x^5+15x^{10}$ for $N_{\\mathrm{train}}=10,100$ and $\\sigma=0,1$. Record your observations.**\n",
    "\n",
    "Now that the true model is no longer contained in all of the model classes, we get different results even with no noise. With no noise, only the 10th order polynomial is able to fit the data perfectly, both in the training set and the testing set. The other models can do okay at predicting if the data falls within the domain of the training set. However, they have difficulty fitting outside of the domain of the training set, especially the linear fit.\n",
    "\n",
    "Once noise is added, we also see new effects. With only 10 points in the training set, the 10th order polynomial actually performs the worst at predicting new data out of all the models. This is because the 10th order model is sensitive to all of the noise in those original 10 points. Once we move to 100 training points, the 10th order model once again performs well and the results are similar to the tests with no noise.\n",
    "\n",
    "**Do better fits lead to better predictions?**\n",
    "\n",
    "Better fits do not always lead to better predictions. This is especially true when the fitting is done on a small number of points as if there is noise even the true model will not have a perfect fit. Better fits also do not always lead to better predictions outside the domain of the training set.\n",
    "\n",
    "**What is the relationship between the true model for generating the data and the model class that has the most predictive power? How is this related to the model complexity? How does this depend on the number of data points $N_{\\mathrm{train}}$ and $\\sigma$?**\n",
    "\n",
    "Generally, the model classes that have the most predictive power contain the true model within them. However, not all model classes containing the true model perform equally well. Simpler models can work better as more complicated model classes can be more sensitive to noise, especially when extrapolating. Overfitting of the data can also be a problem when the number of points in the training set is close to the complexity of the model.\n",
    "\n",
    "**Summarize what you think you learned about the relationship of knowing the true model class and predictive power.**\n",
    "\n",
    "It is hard to predict which model classes will be the most predictive without knowing the true model class. Complicated models can fit training data better but perform terribly at predicting data outside of that domain. If the true model class is not known, extensive testing has to be done across a wide range of outputs in order to ensure that the model class can accurately predict data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
